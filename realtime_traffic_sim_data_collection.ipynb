{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor information:\\nJoey R. Muffoletto\\nUniversity of Texas at Austin\\nAutonomous Systems Group\\njrmuff@utexas.edu\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " '''\n",
    "Author information:\n",
    "Joey R. Muffoletto\n",
    "University of Texas at Austin\n",
    "Autonomous Systems Group\n",
    "jrmuff@utexas.edu\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Joey\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\Joey\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "dill.load_session('realtime_notebook_PURDUE_data_vhub_32_with_queue.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import realtime_manager as rm \n",
    "import graph_manager as gm\n",
    "import reworked_graph as rg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz 2.44.1/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "manager configurations\n",
    "\n",
    "traffic test globals\n",
    "'''\n",
    "\n",
    "USE_PURDUE_DATA = True\n",
    "MAX_ALLOWED_REQUESTS = 5\n",
    "\n",
    "MIN_TOWERS = 2\n",
    "NUM_VERTIHUBS = 32\n",
    "TAU_MAX = 8\n",
    "TAU = 0\n",
    "\n",
    "FREQUENCY_MULTIPLIERS = [2]\n",
    "FREQUENCY_MULTIPLIER = 1\n",
    "\n",
    "LOW_TRAFFIC_MULTIPLIER = 1\n",
    "HIGH_TRAFFIC_MULTIPLIER = 3\n",
    "\n",
    "MIN_LOW_TRAFFIC = 0\n",
    "MAX_LOW_TRAFFIC = LOW_TRAFFIC_MULTIPLIER * int(NUM_VERTIHUBS * FREQUENCY_MULTIPLIER)\n",
    "\n",
    "MIN_HIGH_TRAFFIC = MAX_LOW_TRAFFIC\n",
    "MAX_HIGH_TRAFFIC = HIGH_TRAFFIC_MULTIPLIER * int(NUM_VERTIHUBS * FREQUENCY_MULTIPLIER)\n",
    "\n",
    "MIN_TTL = 3\n",
    "MAX_TTL = 7\n",
    "\n",
    "FLIGHT_SPEED = 60 # m/s\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_EMPTY_STATE = rg.State((),(),{\"0\" : 6})\n",
    "rm.configure_realtime(tau=TAU, override_default_empty_state=DEFAULT_EMPTY_STATE)\n",
    "\n",
    "# HIGH_TRAFFIC_FREQUENCY = .1 # use rand.random() = [0.0, 1.0], or, just add this value until = 1 then reset\n",
    "HIGH_TRAFFIC_TRIGGER = 8\n",
    "NUM_TIME_STEPS = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input generation functions\n",
    "\n",
    "low traffic is some random number between min_low_traffic and max_low_traffic, high traffic is the same with high\n",
    "\n",
    "choose a random tower index everytime we add one\n",
    "\n",
    "high traffic occurs everytime HIGH_TRAFFIC_FREQUENCY * TIME_STEPS is a whole number\n",
    "\n",
    "TTL is a random number between MIN_TTL and MAX_TTL\n",
    "\n",
    "Output format is a dictionary which maps tower index to a list of requests (tuples of preferred port and TTL )\n",
    "\n",
    "This ouput is created per time step, so then the overall input is a list of these dictionaries, with each list index corresponding to the step of the simulation.\n",
    "\n",
    "'''\n",
    "import random\n",
    "random.seed(10)\n",
    "def generate_traffic(min_traffic, max_traffic):\n",
    "    additional_requests_dict = dict()\n",
    "#     counter = 0\n",
    "    requests_to_add = random.randint(min_traffic, max_traffic)\n",
    "    for i in range(requests_to_add):\n",
    "#         if counter == NUM_VERTIHUBS:\n",
    "#             counter = 0\n",
    "#         tower_to_add_to = counter\n",
    "#         counter+=1\n",
    "        tower_to_add_to = random.randint(0, NUM_VERTIHUBS-1) # NUM_VERTIHUBS is OOB\n",
    "        request_to_add = ('no_pref', random.randint(MIN_TTL, MAX_TTL))\n",
    "#         request_to_add = ('no_pref', 5)\n",
    "        if tower_to_add_to in additional_requests_dict:\n",
    "            additional_requests_dict[tower_to_add_to].append(request_to_add)\n",
    "        else:\n",
    "            additional_requests_dict[tower_to_add_to] = [request_to_add]\n",
    "    return additional_requests_dict\n",
    "            \n",
    "def generate_low_traffic():\n",
    "    return generate_traffic(MIN_LOW_TRAFFIC, MAX_LOW_TRAFFIC)\n",
    "    \n",
    "def generate_high_traffic():\n",
    "    return generate_traffic(MIN_HIGH_TRAFFIC, MAX_HIGH_TRAFFIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input Generation\n",
    "'''\n",
    "def generate_inputs():\n",
    "    input = []\n",
    "    high_traffic_counter = 1\n",
    "    for time in range(NUM_TIME_STEPS):\n",
    "        if high_traffic_counter == HIGH_TRAFFIC_TRIGGER:\n",
    "            high_traffic_counter = 1\n",
    "            input.append([generate_high_traffic()])\n",
    "        else:\n",
    "            high_traffic_counter += 1\n",
    "            input.append([generate_low_traffic()])\n",
    "\n",
    "    initial_system = [copy.deepcopy(gm.return_tower(0, 1, [],[6])) for i in range(NUM_VERTIHUBS)]\n",
    "    return initial_system, input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input statistics:\n",
    "'''\n",
    "def generate_input_statistics(input):\n",
    "    requests_per_tower = [0 for i in range(NUM_VERTIHUBS)]\n",
    "    for time_step in range(len(input)):\n",
    "        for i in range(NUM_VERTIHUBS):\n",
    "            if i in input[time_step][0]:\n",
    "                requests_per_tower[i]+=len(input[time_step][0][i])\n",
    "    for i in range(NUM_VERTIHUBS):\n",
    "        print(\"tower \" + str(i) + \"has \" + str(requests_per_tower[i]) + \" requests\")\n",
    "    average_input_frequency = sum([i for i in requests_per_tower])/(float(NUM_TIME_STEPS) * float(NUM_VERTIHUBS))\n",
    "    print(\"average_input_frequency = \" + str(average_input_frequency))\n",
    "    return average_input_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#purdue data methods\n",
    "if USE_PURDUE_DATA == True:\n",
    "    import random\n",
    "    class Purdue_Data_Output:\n",
    "        def __init__(self):\n",
    "            self.num_denied_requests = 0\n",
    "            self.additional_requests_culled = 0\n",
    "            self.max_requests = 0\n",
    "            self.num_expired_requests = 0\n",
    "            \n",
    "    from math import cos, asin, sqrt, pi\n",
    "\n",
    "    def ll_distance(lat1, lon1, lat2, lon2):\n",
    "        p = pi/180\n",
    "        a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "        return 12742 * asin(sqrt(a)) #2*R*asin...   \n",
    "        \n",
    "    # make a list of lists of dictionaries which map tower indicies to requests\n",
    "    def vertiport_statistics(vertiports_data):\n",
    "        '''\n",
    "        vertiport statistics\n",
    "        '''\n",
    "        vertiports_per_vertihub = [0 for i in range(NUM_VERTIHUBS)]\n",
    "        for index, vertiport in vertiports_data.iterrows():\n",
    "#             print((int(vertiport['vertihub'])))\n",
    "            vertiports_per_vertihub[int(vertiport['vertihub'])] += 1\n",
    "        assert(sum(vertiports_per_vertihub) == vertiports_data.shape[0])\n",
    "        vertiports_per_vertihub\n",
    "    \n",
    "    def trip_statistics(trip_data, vertiports_data):\n",
    "        '''\n",
    "        trip statistics\n",
    "        '''\n",
    "        arrivals_per_vertihub = [[] for i in range(NUM_VERTIHUBS)]\n",
    "        flight_popularity = dict()\n",
    "        # get the trip arrival time per vertihub\n",
    "        for index, trip in trip_data.iterrows():\n",
    "            #calculate time of arrival (begin at trip.time), calculated between vertiports\n",
    "            origin_port = vertiports_data.iloc[trip['OriginVertiport']]\n",
    "            destination_port = vertiports_data.iloc[trip['DestinationVertiport']]\n",
    "            if((origin_port['vertiport'],destination_port['vertiport']) not in flight_popularity.keys()):\n",
    "                flight_popularity[(origin_port['vertiport'], destination_port['vertiport'])] = 1\n",
    "            else:\n",
    "                flight_popularity[(origin_port['vertiport'], destination_port['vertiport'])] += 1\n",
    "            #trip distance in kilometers\n",
    "            dist = ll_distance(origin_port['lat'], origin_port['long'], destination_port['lat'], destination_port['long'])\n",
    "            travel_time = (dist*1000)/FLIGHT_SPEED # (km * 1000)/(m/s) -> seconds\n",
    "            arrival_time = trip['Time'] + travel_time #takeoff time + travel_time = arrival time\n",
    "            #add arrival time to vertihub\n",
    "            arrivals_per_vertihub[trip['DestinationVertihub']].append(arrival_time)\n",
    "        \n",
    "        # for each list of vertihub, calculate the minimum, average, and maximum distance between arrivals\n",
    "        differences_per_vertihub = []\n",
    "        # simulated_arrivals\n",
    "        for arrivals_list in arrivals_per_vertihub:\n",
    "            sorted_arrivals = sorted(arrivals_list)\n",
    "            if(len(sorted_arrivals) <= 1):\n",
    "                continue\n",
    "            differences = []\n",
    "            for index in range(len(sorted_arrivals)-1):\n",
    "                differences.append(sorted_arrivals[index+1] - sorted_arrivals[index])\n",
    "            print(sorted_arrivals)\n",
    "            differences_per_vertihub.append(sorted(differences))\n",
    "        \n",
    "        for index, differences in enumerate(differences_per_vertihub):\n",
    "            print('vertihub ' + str(index) + ' has minimum arrival difference of ' + str(differences[0]))\n",
    "            print('vertihub ' + str(index) + ' has average arrival difference of ' + str(sum(differences)/len(differences)))\n",
    "            print('vertihub ' + str(index) + ' has maximum arrival difference of ' + str(differences[len(differences)-1]))\n",
    "            print('vertihub ' + str(index) + ' has ' + str(len(differences)) + ' trips')\n",
    "            print('')\n",
    "           \n",
    "        \n",
    "        formatted_flight_popularity = [(flight_popularity[key], key) for key in flight_popularity.keys()]\n",
    "        print('Most popular flight path ' + str((sorted(formatted_flight_popularity))[len(formatted_flight_popularity)-1]))\n",
    "        print('Total flight paths ' + str(len(formatted_flight_popularity)))\n",
    "        avg = sum([num for num, thing in formatted_flight_popularity])/len(formatted_flight_popularity)\n",
    "        print('Mean requests per flight path ' + str(avg))\n",
    "    \n",
    "    def load_vertiport_data(display_stats=True):\n",
    "        vertiports_data = pd.read_csv('data/Realtime/OpsLimits/vertiports_' + str(NUM_VERTIHUBS) + '.csv')\n",
    "        if display_stats:\n",
    "            vertiport_statistics(vertiports_data)\n",
    "        return vertiports_data\n",
    "    def load_trip_data(vertiports_data, display_stats=True):\n",
    "        trip_data = pd.read_csv('data/Realtime/OpsLimits/trips_32.csv')\n",
    "        trip_data.drop(trip_data[trip_data.OriginVertihub == trip_data.DestinationVertihub].index, inplace=True) #drop trips where the origin and destination are the same vertiport, reduced from 4801 trips to 1692 with 10 vertiports\n",
    "        if display_stats:\n",
    "            trip_statistics(trip_data, vertiports_data)\n",
    "        return trip_data\n",
    "    \n",
    "    def format_purdue_dataset():\n",
    "        vertiports_data = load_vertiport_data()\n",
    "        trip_data = load_trip_data(vertiports_data)\n",
    "        arrivals_per_vertihub = [[] for i in range(NUM_VERTIHUBS)]\n",
    "        # get the trip arrival time per vertihub\n",
    "        latest_arrival_time = -1\n",
    "        for index, trip in trip_data.iterrows():\n",
    "            #calculate time of arrival (begin at trip.time), calculated between vertiports\n",
    "            origin_port = vertiports_data.iloc[trip['OriginVertiport']]\n",
    "            destination_port = vertiports_data.iloc[trip['DestinationVertiport']]\n",
    "            #trip distance in kilometers\n",
    "            dist = ll_distance(origin_port['lat'], origin_port['long'], destination_port['lat'], destination_port['long'])\n",
    "            travel_time = (dist*1000)/FLIGHT_SPEED # (km * 1000)/(m/s) -> seconds\n",
    "            arrival_time = trip['Time'] + travel_time #takeoff time + travel_time = arrival time\n",
    "            if(arrival_time > latest_arrival_time):\n",
    "                latest_arrival_time = arrival_time\n",
    "            #add arrival time to vertihub\n",
    "            arrivals_per_vertihub[trip['DestinationVertihub']].append((int(arrival_time), \n",
    "                                                                       int(destination_port['vertihub']), \n",
    "                                                                           int(destination_port['vertiport']\n",
    "                                                                              )))\n",
    "        #build input list\n",
    "        input = [[dict()] for i in range(int(latest_arrival_time)+1)]\n",
    "        for arrivals in arrivals_per_vertihub:\n",
    "            for arrival in arrivals:\n",
    "                arrival_time, destination_hub, destination_port = arrival\n",
    "                randomized_TTL = random.randint(MIN_TTL, MAX_TTL)\n",
    "                to_add = (('' + str(destination_port)), randomized_TTL) # (destination port, time to land)\n",
    "                if destination_hub not in input[arrival_time][0].keys():\n",
    "                    input[arrival_time][0][destination_hub] = []\n",
    "                #TODO: CHANGE THIS EVENTUALLY!!!!!\n",
    "                if(len(input[arrival_time][0][destination_hub]) < 100): \n",
    "                    input[arrival_time][0][destination_hub].append(to_add)\n",
    "          \n",
    "        #build vertihub list\n",
    "        initial_vertihubs = []\n",
    "        for vertihub_index in range(NUM_VERTIHUBS):\n",
    "            #TODO: fix this... replace 'no_pref' with vertiport_index and figure out the issue with your legacy code! :)\n",
    "            port_dict = {'no_pref' : 3 for vertiport_index, trip in vertiports_data.iterrows() if trip['vertihub'] == vertihub_index}\n",
    "            accepted_requests_per_time_step = 1\n",
    "            request_vector = []\n",
    "            time_vec = []\n",
    "            vertihub = gm.return_tower_specific(port_dict, accepted_requests_per_time_step, request_vector, time_vec)\n",
    "            initial_vertihubs.append(vertihub)\n",
    "        return initial_vertihubs, input\n",
    "                          \n",
    "                \n",
    "                \n",
    "                                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "data collection functions\n",
    "'''\n",
    "def run_realtime_data_collection(initial_system_copy, input_copy, PDOs=None):\n",
    "    gm.reset_globals()\n",
    "    global MAX_ALLOWED_REQUESTS\n",
    "    \n",
    "    _completed_states_per_tau = [[] for i in range(TAU_MAX)]\n",
    "    _timing_info_per_tau = [[] for i in range(TAU_MAX)]\n",
    "    for _tau in range(TAU_MAX):\n",
    "        rm.configure_realtime(tau=_tau, override_default_empty_state=DEFAULT_EMPTY_STATE)\n",
    "        traces = None\n",
    "        timings = None\n",
    "        if not USE_PURDUE_DATA:\n",
    "            traces, timings = rm.main_loop(initial_system_copy, copy.deepcopy(input_copy))\n",
    "        else:\n",
    "            #NOTE: when using the Purdue data, we will be deleting requests that were denied during runs with higher TAUs.\n",
    "            # that is why we are no longer making a copy of the input_copy, and instead passing the direct reference to be modified\n",
    "            #NOTE: with request queueing, the previous comment is deprecated. all requests will be kept, but a queue will be used in the event of overflow\n",
    "            print(\"INPUT STATISTICS FOR TAU : \" + str(_tau))\n",
    "            generate_input_statistics(input_copy)\n",
    "            curr_PDO = PDOs[_tau]\n",
    "            traces, timings = rm.main_loop(initial_system_copy, copy.deepcopy(input_copy), MAX_ALLOWED_REQUESTS=MAX_ALLOWED_REQUESTS, Purdue_Data_Output=curr_PDO)\n",
    "#             if _tau == 0:\n",
    "            input_copy = curr_PDO.additional_requests_culled\n",
    "#                 MAX_ALLOWED_REQUESTS = 100 #because fuck the next guys\n",
    "        _completed_states_per_tau[_tau] = copy.deepcopy(traces)\n",
    "        _timing_info_per_tau[_tau] = copy.deepcopy(timings)\n",
    "        gm.reset_globals()\n",
    "    return _completed_states_per_tau, _timing_info_per_tau\n",
    "    #     print(traces)\n",
    "        # _completed_states, _timing_info = rm.main_loop(initial_system_copy, input_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data collection pipeline\n",
    "'''\n",
    "if USE_PURDUE_DATA:\n",
    "    FREQUENCY_MULTIPLIERS = [1]\n",
    "    TAU_MAX = 1 #just run it for the TAU = 0 case.\n",
    "\n",
    "_completed_states_per_tau_per_freq = {freq : [] for freq in FREQUENCY_MULTIPLIERS}\n",
    "_timing_info_per_tau_per_freq = {freq : [] for freq in FREQUENCY_MULTIPLIERS}\n",
    "_actual_frequencies_per_freq = {freq : [] for freq in FREQUENCY_MULTIPLIERS}\n",
    "NUM_TRIALS = 5\n",
    "for freq in FREQUENCY_MULTIPLIERS:\n",
    "    if not USE_PURDUE_DATA:\n",
    "        # setting the frequency multipliers\n",
    "        MAX_LOW_TRAFFIC = LOW_TRAFFIC_MULTIPLIER * int(NUM_VERTIHUBS * freq)\n",
    "    \n",
    "        MIN_HIGH_TRAFFIC = MAX_LOW_TRAFFIC\n",
    "        MAX_HIGH_TRAFFIC = HIGH_TRAFFIC_MULTIPLIER * int(NUM_VERTIHUBS * freq)\n",
    "        for i in range(NUM_TRIALS):\n",
    "            initial_system, input = generate_inputs()\n",
    "            input_frequency = generate_input_statistics(input) \n",
    "            _completed_states_per_tau, _timing_info_per_tau = run_realtime_data_collection(copy.deepcopy(initial_system), copy.deepcopy(input))\n",
    "            _completed_states_per_tau_per_freq[freq].append(_completed_states_per_tau)\n",
    "            _timing_info_per_tau_per_freq[freq].append(_timing_info_per_tau)\n",
    "            _actual_frequencies_per_freq[freq].append(input_frequency)\n",
    "    else:\n",
    "        initial_system, _input = format_purdue_dataset() #Purdue dataset\n",
    "        # print(_input)\n",
    "        PDOs = [Purdue_Data_Output() for i in range(TAU_MAX)]\n",
    "        input_frequency = generate_input_statistics(_input) \n",
    "        _completed_states_per_tau, _timing_info_per_tau = run_realtime_data_collection(copy.deepcopy(initial_system), copy.deepcopy(_input), PDOs=PDOs)\n",
    "        _completed_states_per_tau_per_freq[freq].append(_completed_states_per_tau)\n",
    "        _timing_info_per_tau_per_freq[freq].append(_timing_info_per_tau)\n",
    "        _actual_frequencies_per_freq[freq].append(input_frequency)\n",
    "        \n",
    "        for _tau, pdo in enumerate(PDOs):\n",
    "            print(\"DENIED REQUESTS, MAX_REQUESTS, TAU \" + str(pdo.num_denied_requests) + \", \" + str(pdo.max_requests) + \", \" + str(_tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_TRIALS = 5\n",
    "if USE_PURDUE_DATA:\n",
    "    NUM_TRIALS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data buffer\n",
    "'''\n",
    "completed_states_per_tau_per_freq = copy.deepcopy(_completed_states_per_tau_per_freq)\n",
    "timing_info_per_tau_per_freq = copy.deepcopy(_timing_info_per_tau_per_freq)\n",
    "actual_frequencies_per_freq = copy.deepcopy(_actual_frequencies_per_freq)\n",
    "\n",
    "# print (_actual_frequencies_per_freq)\n",
    "# print (_completed_states_per_tau_per_freq)\n",
    "\n",
    "# import dill\n",
    "# if not USE_PURDUE_DATA:\n",
    "#     dill.dump_session('realtime_notebook_store_highest_freq.db')\n",
    "# else:\n",
    "#     dill.dump_session('realtime_notebook_PURDUE_data_vhub_32_with_queue.db')\n",
    "#     dill.dump_session('realtime_notebook_PURDUE_data_vertihubs_'+str(NUM_VERTIHUBS)+'.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (sum(timing_info_per_tau_per_freq[1.5][0][0])/len(timing_info_per_tau_per_freq[1.5][0][0]))\n",
    "# print (len(timing_info_per_tau_per_freq[1.5][0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "percentage of valid states\n",
    "'''\n",
    "def get_percent_valid(completed_states):\n",
    "    percent_valid = [0 for i in range(len(completed_states[0]))]\n",
    "    percent_valid_per_tau = [copy.copy(percent_valid) for i in range(TAU_MAX)]\n",
    "    for tau,completed in enumerate(completed_states):\n",
    "        print(completed)\n",
    "        print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "        for index, com in enumerate(completed):\n",
    "            for state in com:\n",
    "                if(\"VALID\" in state.labels):\n",
    "                    percent_valid_per_tau[tau][index] += 1\n",
    "\n",
    "    for tower_cost in percent_valid_per_tau:\n",
    "        print(tower_cost)\n",
    "    return percent_valid_per_tau\n",
    "    \n",
    "# actual_percentages = [(i/len(completed_states[0])) for i in percent_valid]\n",
    "# print(actual_percentages)\n",
    "'''\n",
    "mvp_output\n",
    "'''\n",
    "def get_mvp_output_per_tower_per_tau(completed_states):\n",
    "    mvp_output_per_tower_per_tau = [rm.get_mvp_output(completed) for completed in completed_states]\n",
    "    for tau in mvp_output_per_tower_per_tau:\n",
    "        for output in tau:\n",
    "            gm.print_formatted_cost(output[0],format_override=True)\n",
    "    #     output[3].plot()\n",
    "    #     gm.print_formatted_trace_path(output[1])\n",
    "    return mvp_output_per_tower_per_tau\n",
    "\n",
    "'''\n",
    "calculating heuristic cost\n",
    "'''\n",
    "def get_heuristic_cost_per_tau(completed_states):\n",
    "    heurstic_cost_per_tau = [copy.copy(percent_valid) for i in range(TAU_MAX)]\n",
    "    for tau,completed in enumerate(completed_states):\n",
    "        for index, com in enumerate(completed):\n",
    "            for state in com:\n",
    "                for req in state.request_vector:\n",
    "                    if(req == \"wrong_tower\"):\n",
    "                        heurstic_cost_per_tau[tau][index] += 1\n",
    "    for heuristic_cost in heurstic_cost_per_tau:\n",
    "        print(heuristic_cost)\n",
    "    return heuristic_cost_per_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "table for runtime data\n",
    "\n",
    "most important data is synthesis per time per timestep. \n",
    "Ill just do runtime vs TAU and frequency, one for each trial\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_runtime_table(timing_info, input_frequencies, trial_num, iteration):\n",
    "    row_headers = [''+str(freq) for freq in input_frequencies]\n",
    "    column_headers = [''+str(tau) for tau in range(TAU_MAX)]\n",
    "    cell_text = []\n",
    "    for freq in FREQUENCY_MULTIPLIERS:\n",
    "        cell_text.append(timing_info[freq])\n",
    "        print(cell_text)\n",
    "#         cell_text.append([0 for i in range(TAU_MAX)])\n",
    "    the_table = plt.table(cellText=cell_text, rowLabels=row_headers, colLabels=column_headers, loc='center')\n",
    "    the_table.scale(1, 1.5)\n",
    "    the_table.set_fontsize(25)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.box(on=None)\n",
    "#     plt.show()\n",
    "#     fig = plt.gcf()\n",
    "    if USE_PURDUE_DATA:\n",
    "        plt.savefig('data/Realtime/OpsLimits/runtime_vs_TAU_and_freq_trial_' + str(trial_num) + '_iteration_' + str(iteration) + '.png',bbox_inches='tight',dpi=216)    \n",
    "    else:\n",
    "        plt.savefig('data/Realtime/runtime_vs_TAU_and_freq_trial_' + str(trial_num) + '_iteration_' + str(iteration) + '.png',bbox_inches='tight',dpi=216)    \n",
    "    \n",
    "    return plt\n",
    "\n",
    "def get_data_per_trial(full_timing_info, full_input_frequencies, trial_index):\n",
    "    stub_timing_info = {freq : [0 for i in range(TAU_MAX)] for freq in FREQUENCY_MULTIPLIERS}\n",
    "    for freq in full_timing_info:        \n",
    "        for TAU in range(len(full_timing_info[freq][trial_index])):\n",
    "            timings_without_zeros = [t for t in full_timing_info[freq][trial_index][TAU] if t != 0]\n",
    "            stub_timing_info[freq][TAU] += sum(timings_without_zeros)/len(timings_without_zeros)\n",
    "    stub_input_frequencies = {freq: 0 for freq in FREQUENCY_MULTIPLIERS}\n",
    "    for freq in full_input_frequencies:\n",
    "        stub_input_frequencies[freq] = full_input_frequencies[freq][trial_index]\n",
    "    return stub_timing_info, stub_input_frequencies\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_timing_info = {freq : [0 for i in range(TAU_MAX)] for freq in FREQUENCY_MULTIPLIERS}\n",
    "for trial in range(0, NUM_TRIALS):\n",
    "    timing_info, input_freq = get_data_per_trial(timing_info_per_tau_per_freq, actual_frequencies_per_freq, trial)\n",
    "#     print(timing_info)\n",
    "    for key in timing_info:\n",
    "        for index,num in enumerate(timing_info[key]):\n",
    "            timing_info[key][index] = round(num,3)\n",
    "    generate_runtime_table(timing_info, input_freq, trial, 2)\n",
    "#     for key in timing_info:\n",
    "#         for index,num in enumerate(timing_info[key]):\n",
    "#             average_timing_info[key][index] += num\n",
    "            \n",
    "for key in timing_info:\n",
    "    for index,num in enumerate(timing_info[key]):\n",
    "        average_timing_info[key][index] = round(num,3)\n",
    "# fig = generate_runtime_table(average_timing_info, input_freq, trial)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "data buffer for mvp output\n",
    "'''\n",
    "completed_states = completed_states_per_tau_per_freq[1][0]\n",
    "percent_valid = [0 for i in range(len(completed_states[0]))]\n",
    "percent_valid_per_tau = [copy.copy(percent_valid) for i in range(TAU_MAX)]\n",
    "# wrong_towers_per_tau = [copy.copy(percent_valid) for i in range(TAU_MAX)]\n",
    "wrong_tower_per_time_step = [0 for i in range(len(completed_states[0][0]))]\n",
    "number_requests_per_TAU = [0 for i in range(len(completed_states[0]))]\n",
    "# print(completed_states[0][0])\n",
    "# print(completed_states_per_tau_per_freq[1][0][0][0][0]) freq, something, tau, tower, state\n",
    "states_per_tau = [0 for i in range(TAU_MAX)]\n",
    "for tau,completed in enumerate(completed_states): # tau\n",
    "    for index, com in enumerate(completed): # vertihub\n",
    "        for time_step, state in enumerate(com):\n",
    "            if(\"VALID\" in state.labels):\n",
    "                percent_valid_per_tau[tau][index] += 1\n",
    "            states_per_tau[tau] += 1\n",
    "            for req in state.request_vector:\n",
    "                if req == 'wrong_tower':\n",
    "                     wrong_tower_per_time_step[time_step] += 1\n",
    "#                      wrong_towers_per_tau[tau][time_step] += 1\n",
    "                    \n",
    "print(completed_states[0][0])\n",
    "# sums = [sum(i) for i in wrong_towers_per_tau]\n",
    "# print(sums)          \n",
    "other_sums = [sum(i)/states_per_tau[tau] for tau, i in enumerate(percent_valid_per_tau)]\n",
    "print(other_sums)\n",
    "\n",
    "    \n",
    "\n",
    "# for tower_cost in percent_valid_per_tau:\n",
    "#     print(tower_cost)\n",
    "# mvp_output_per_tower_per_tau_copy = copy.deepcopy(mvp_output_per_tower_per_tau)\n",
    "# for tau in mvp_output_per_tower_per_tau_copy:\n",
    "#     for output in tau:\n",
    "#         gm.print_formatted_cost(output[0],format_override=True)\n",
    "# for timings in timing_info:\n",
    "#     print(sum(timings))\n",
    "#     print (timings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "More data processing\n",
    "'''\n",
    "# sum up the negative costs for the tower (get total expiration value)\n",
    "cost_per_tau_over_time = [[] for i in range(TAU_MAX)]\n",
    "for index,tau in enumerate(completed_states):\n",
    "    sum_cost = 0\n",
    "    for time_step in range(len(completed_states[0][0])):\n",
    "        for tower in tau:\n",
    "            for expiration in tower[time_step].time_vector:\n",
    "                if expiration < 0:\n",
    "                    sum_cost -= expiration \n",
    "#                     sum_cost +=1\n",
    "        cost_per_tau_over_time[index].append(sum_cost)\n",
    "cost_per_tau_over_time = cost_per_tau_over_time\n",
    "print(cost_per_tau_over_time)\n",
    "'''\n",
    "first plot:\n",
    "cummulative expiration cost vs. time step\n",
    "label with periods of high traffic and low traffic\n",
    "do a line per tau\n",
    "'''\n",
    "x = [i+1 for i in range(len(completed_states[0][0]))]\n",
    "cummulative_wrong_tower_error = [0 for i in range(len(wrong_tower_per_time_step))]\n",
    "for idx, i in enumerate(wrong_tower_per_time_step):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    cummulative_wrong_tower_error[idx] = i + cummulative_wrong_tower_error[idx-1]\n",
    "print(wrong_tower_per_time_step)\n",
    "plt.plot(x, cummulative_wrong_tower_error)\n",
    "# for tau_num, tau in enumerate(cost_per_tau_over_time):\n",
    "#     plt.plot(x, tau, label = \"TAU = \" + str(tau_num))\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Cumulative Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "second plot:\n",
    "cummulative expiration cost vs. time step\n",
    "label with periods of high traffic and low traffic\n",
    "do a line per tau\n",
    "also include a line for the traffic over time\n",
    "'''\n",
    "\n",
    "cum_input_len_over_time = [0 for i in x]\n",
    "for time_step in range(len(input)):\n",
    "    if time_step > 0:\n",
    "        cum_input_len_over_time[time_step] = cum_input_len_over_time[time_step-1]  \n",
    "    for key in input[time_step][0].keys():\n",
    "        cum_input_len_over_time[time_step] += len(input[time_step][0][key])\n",
    "print (cum_input_len_over_time)\n",
    "for i in range(len(input), len(x)):\n",
    "    cum_input_len_over_time[i] = cum_input_len_over_time[i-1]\n",
    "\n",
    "fig,ax1 = plt.subplots()\n",
    "# print (cost_per_tau_over_time[4])\n",
    "for tau_num, tau in enumerate(cost_per_tau_over_time):\n",
    "    ax1.plot(x, tau, label = \"TAU = \" + str(tau_num))\n",
    "plt.legend()\n",
    "ax1.set_xlabel('Time Step')\n",
    "ax1.set_ylabel('Cumulative Error')\n",
    "input_len_over_time = [0 for i in x]\n",
    "for time_step in range(len(input)):\n",
    "    for key in input[time_step][0].keys():\n",
    "        input_len_over_time[time_step] += len(input[time_step][0][key])\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x, input_len_over_time, color='r', alpha=0.1)\n",
    "# ax2.plot(x, cum_input_len_over_time, 'r.')\n",
    "ax2.set_ylabel('Additional Requests')\n",
    "ax2.tick_params('y',colors='r')\n",
    "if USE_PURDUE_DATA:\n",
    "    plt.savefig('data/Realtime/OpsLimits/cumulative_error_over_time_with_requests' + str(randrange(0,100000)) + '.png',dpi=216)    \n",
    "else:\n",
    "    plt.savefig('data/Realtime/cumulative_error_over_time_with_requests' + str(randrange(0,100000)) + '.png',dpi=216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "second plot:\n",
    "traffic over time\n",
    "'''\n",
    "input_len_over_time = [0 for i in x]\n",
    "for time_step in range(len(input)):\n",
    "    for key in input[time_step][0].keys():\n",
    "        input_len_over_time[time_step] += len(input[time_step][0][key])\n",
    "plt.bar(x, input_len_over_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "timings data\n",
    "'''\n",
    "for timings in timing_info:\n",
    "#     print(timings)\n",
    "    print(sum(timings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
