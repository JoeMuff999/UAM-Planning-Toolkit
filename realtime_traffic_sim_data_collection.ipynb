{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nAuthor information:\\nJoey R. Muffoletto\\nUniversity of Texas at Austin\\nAutonomous Systems Group\\njrmuff@utexas.edu\\n'"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    " '''\n",
    "Author information:\n",
    "Joey R. Muffoletto\n",
    "University of Texas at Austin\n",
    "Autonomous Systems Group\n",
    "jrmuff@utexas.edu\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "# dill.load_session('realtime_notebook_PURDUE_data_vhub_32_with_queue.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import realtime_manager as rm \n",
    "import graph_manager as gm\n",
    "import reworked_graph as rg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz 2.44.1/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "manager configurations\n",
    "\n",
    "traffic test globals\n",
    "'''\n",
    "\n",
    "USE_PURDUE_DATA = True\n",
    "\n",
    "MAX_ALLOWED_REQUESTS = 5\n",
    "\n",
    "MIN_TOWERS = 2\n",
    "NUM_VERTIHUBS = 16\n",
    "MINUTES_OF_DATA = 1\n",
    "ADDITIONAL_VERTIHUBS = 0 # used for file path purposes. leave num_vertihubs as the total number of vertihubs (including the additional)\n",
    "\n",
    "TAU_MAX = 8\n",
    "TAU = 0\n",
    "\n",
    "FREQUENCY_MULTIPLIERS = [2]\n",
    "FREQUENCY_MULTIPLIER = 1\n",
    "\n",
    "LOW_TRAFFIC_MULTIPLIER = 1\n",
    "HIGH_TRAFFIC_MULTIPLIER = 3\n",
    "\n",
    "MIN_LOW_TRAFFIC = 0\n",
    "MAX_LOW_TRAFFIC = LOW_TRAFFIC_MULTIPLIER * int(NUM_VERTIHUBS * FREQUENCY_MULTIPLIER)\n",
    "\n",
    "MIN_HIGH_TRAFFIC = MAX_LOW_TRAFFIC\n",
    "MAX_HIGH_TRAFFIC = HIGH_TRAFFIC_MULTIPLIER * int(NUM_VERTIHUBS * FREQUENCY_MULTIPLIER)\n",
    "\n",
    "MIN_TTL = 3\n",
    "MAX_TTL = 7\n",
    "\n",
    "FLIGHT_SPEED = 60 # m/s\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_EMPTY_STATE = rg.State((),(),{\"0\" : 6})\n",
    "rm.configure_realtime(tau=TAU, override_default_empty_state=DEFAULT_EMPTY_STATE)\n",
    "\n",
    "# HIGH_TRAFFIC_FREQUENCY = .1 # use rand.random() = [0.0, 1.0], or, just add this value until = 1 then reset\n",
    "HIGH_TRAFFIC_TRIGGER = 8\n",
    "NUM_TIME_STEPS = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIPS_PATH = \"\"\n",
    "VERTIPORTS_PATH = \"\"\n",
    "if USE_PURDUE_DATA:\n",
    "    TRIPS_PATH = 'data/Realtime/OpsLimits/trips_' + str(NUM_VERTIHUBS) + '_minutes-' + str(MINUTES_OF_DATA)\n",
    "    if ADDITIONAL_VERTIHUBS > 0:\n",
    "        TRIPS_PATH += '_additional_vhubs_' + str(ADDITIONAL_VERTIHUBS)\n",
    "    TRIPS_PATH += '.csv'\n",
    "\n",
    "    VERTIPORTS_PATH = 'data/Realtime/OpsLimits/vertiports_' + str(NUM_VERTIHUBS)\n",
    "    if ADDITIONAL_VERTIHUBS > 0:\n",
    "        VERTIPORTS_PATH += '_additional_vhubs_' + str(ADDITIONAL_VERTIHUBS)\n",
    "    VERTIPORTS_PATH += '.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input generation functions\n",
    "\n",
    "low traffic is some random number between min_low_traffic and max_low_traffic, high traffic is the same with high\n",
    "\n",
    "choose a random tower index everytime we add one\n",
    "\n",
    "high traffic occurs everytime HIGH_TRAFFIC_FREQUENCY * TIME_STEPS is a whole number\n",
    "\n",
    "TTL is a random number between MIN_TTL and MAX_TTL\n",
    "\n",
    "Output format is a dictionary which maps tower index to a list of requests (tuples of preferred port and TTL )\n",
    "\n",
    "This ouput is created per time step, so then the overall input is a list of these dictionaries, with each list index corresponding to the step of the simulation.\n",
    "\n",
    "'''\n",
    "import random\n",
    "random.seed(10)\n",
    "def generate_traffic(min_traffic, max_traffic):\n",
    "    additional_requests_dict = dict()\n",
    "#     counter = 0\n",
    "    requests_to_add = random.randint(min_traffic, max_traffic)\n",
    "    for i in range(requests_to_add):\n",
    "#         if counter == NUM_VERTIHUBS:\n",
    "#             counter = 0\n",
    "#         tower_to_add_to = counter\n",
    "#         counter+=1\n",
    "        tower_to_add_to = random.randint(0, NUM_VERTIHUBS-1) # NUM_VERTIHUBS is OOB\n",
    "        request_to_add = ('no_pref', random.randint(MIN_TTL, MAX_TTL))\n",
    "#         request_to_add = ('no_pref', 5)\n",
    "        if tower_to_add_to in additional_requests_dict:\n",
    "            additional_requests_dict[tower_to_add_to].append(request_to_add)\n",
    "        else:\n",
    "            additional_requests_dict[tower_to_add_to] = [request_to_add]\n",
    "    return additional_requests_dict\n",
    "            \n",
    "def generate_low_traffic():\n",
    "    return generate_traffic(MIN_LOW_TRAFFIC, MAX_LOW_TRAFFIC)\n",
    "    \n",
    "def generate_high_traffic():\n",
    "    return generate_traffic(MIN_HIGH_TRAFFIC, MAX_HIGH_TRAFFIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input Generation\n",
    "'''\n",
    "def generate_inputs():\n",
    "    input = []\n",
    "    high_traffic_counter = 1\n",
    "    for time in range(NUM_TIME_STEPS):\n",
    "        if high_traffic_counter == HIGH_TRAFFIC_TRIGGER:\n",
    "            high_traffic_counter = 1\n",
    "            input.append([generate_high_traffic()])\n",
    "        else:\n",
    "            high_traffic_counter += 1\n",
    "            input.append([generate_low_traffic()])\n",
    "\n",
    "    initial_system = [copy.deepcopy(gm.return_tower(0, 1, [],[6])) for i in range(NUM_VERTIHUBS)]\n",
    "    return initial_system, input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input statistics:\n",
    "'''\n",
    "def generate_input_statistics(input):\n",
    "    requests_per_tower = [0 for i in range(NUM_VERTIHUBS)]\n",
    "    for time_step in range(len(input)):\n",
    "        for i in range(NUM_VERTIHUBS):\n",
    "            if i in input[time_step][0]:\n",
    "                requests_per_tower[i]+=len(input[time_step][0][i])\n",
    "    for i in range(NUM_VERTIHUBS):\n",
    "        print(\"tower \" + str(i) + \"has \" + str(requests_per_tower[i]) + \" requests\")\n",
    "    average_input_frequency = sum([i for i in requests_per_tower])/(float(NUM_TIME_STEPS) * float(NUM_VERTIHUBS))\n",
    "    print(\"average_input_frequency = \" + str(average_input_frequency))\n",
    "    return average_input_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#purdue data methods\n",
    "if USE_PURDUE_DATA == True:\n",
    "    import random\n",
    "    class Purdue_Data_Output:\n",
    "        def __init__(self):\n",
    "            self.num_denied_requests = 0\n",
    "            self.additional_requests_culled = 0\n",
    "            self.max_requests = 0\n",
    "            self.num_expired_requests = 0\n",
    "            self.expired_requests = [] # tracks the ttl and vertihub id of each expired request when it was popped off of the queue\n",
    "            self.average_queue_size = [0 for i in range(NUM_VERTIHUBS)]\n",
    "            \n",
    "    from math import cos, asin, sqrt, pi\n",
    "\n",
    "    def ll_distance(lat1, lon1, lat2, lon2):\n",
    "        p = pi/180\n",
    "        a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "        return 12742 * asin(sqrt(a)) #2*R*asin...   \n",
    "        \n",
    "    # make a list of lists of dictionaries which map tower indicies to requests\n",
    "    def vertiport_statistics(vertiports_data, _file=sys.stdout):\n",
    "        '''\n",
    "        vertiport statistics\n",
    "        this data is bogus as of 2/18 -> this is because we are not accounting for how the vertiports now can be mapped to multiple vertihubs if we are using additional vhubs\n",
    "        '''\n",
    "        vertiports_per_vertihub = [0 for i in range(NUM_VERTIHUBS)]\n",
    "        for index, vertiport in vertiports_data.iterrows():\n",
    "#             print((int(vertiport['vertihub'])))\n",
    "            vertiports_per_vertihub[int(vertiport['vertihub_0'])] += 1\n",
    "        assert(sum(vertiports_per_vertihub) == vertiports_data.shape[0])\n",
    "        vertiports_per_vertihub\n",
    "    \n",
    "    def trip_statistics(trip_data, vertiports_data, _file=sys.stdout):\n",
    "        '''\n",
    "        trip statistics\n",
    "        '''\n",
    "        arrivals_per_vertihub = [[] for i in range(NUM_VERTIHUBS)]\n",
    "        flight_popularity = dict()\n",
    "        # get the trip arrival time per vertihub\n",
    "        for index, trip in trip_data.iterrows():\n",
    "            #calculate time of arrival (begin at trip.time), calculated between vertiports\n",
    "            origin_port = vertiports_data.iloc[trip['OriginVertiport']]\n",
    "            destination_port = vertiports_data.iloc[trip['DestinationVertiport']]\n",
    "            if((origin_port['vertiport'],destination_port['vertiport']) not in flight_popularity.keys()):\n",
    "                flight_popularity[(origin_port['vertiport'], destination_port['vertiport'])] = 1\n",
    "            else:\n",
    "                flight_popularity[(origin_port['vertiport'], destination_port['vertiport'])] += 1\n",
    "            #trip distance in kilometers\n",
    "            dist = ll_distance(origin_port['lat'], origin_port['long'], destination_port['lat'], destination_port['long'])\n",
    "            travel_time = (dist*1000)/FLIGHT_SPEED # (km * 1000)/(m/s) -> seconds\n",
    "            arrival_time = trip['Time'] + travel_time #takeoff time + travel_time = arrival time\n",
    "            #add arrival time to vertihub\n",
    "            arrivals_per_vertihub[trip['DestinationVertihub']].append(arrival_time)\n",
    "        \n",
    "        # for each list of vertihub, calculate the minimum, average, and maximum distance between arrivals\n",
    "        differences_per_vertihub = []\n",
    "        # simulated_arrivals\n",
    "        for arrivals_list in arrivals_per_vertihub:\n",
    "            sorted_arrivals = sorted(arrivals_list)\n",
    "            if(len(sorted_arrivals) <= 1):\n",
    "                continue\n",
    "            differences = []\n",
    "            for index in range(len(sorted_arrivals)-1):\n",
    "                differences.append(sorted_arrivals[index+1] - sorted_arrivals[index])\n",
    "            # print(sorted_arrivals)\n",
    "            differences_per_vertihub.append(sorted(differences))\n",
    "        \n",
    "        for index, differences in enumerate(differences_per_vertihub):\n",
    "            print('vertihub ' + str(index) + ' has minimum arrival difference of ' + str(differences[0]), file=_file)\n",
    "            print('vertihub ' + str(index) + ' has average arrival difference of ' + str(sum(differences)/len(differences)), file=_file)\n",
    "            print('vertihub ' + str(index) + ' has maximum arrival difference of ' + str(differences[len(differences)-1]), file=_file)\n",
    "            print('vertihub ' + str(index) + ' has ' + str(len(differences)) + ' trips', file=_file)\n",
    "            print('', file=_file)\n",
    "           \n",
    "        \n",
    "        formatted_flight_popularity = [(flight_popularity[key], key) for key in flight_popularity.keys()]\n",
    "        print('Most popular flight path ' + str((sorted(formatted_flight_popularity))[len(formatted_flight_popularity)-1]), file=_file)\n",
    "        print('Total flight paths ' + str(len(formatted_flight_popularity)), file=_file)\n",
    "        avg = sum([num for num, thing in formatted_flight_popularity])/len(formatted_flight_popularity)\n",
    "        print('Mean requests per flight path ' + str(avg), file=_file)\n",
    "\n",
    "        formatted_vertihub_popularity = [(len(differences_per_vertihub[i]), i) for i in range(len(differences_per_vertihub))]\n",
    "\n",
    "        print('Sorted vertihub popularity: ' + str(sorted(formatted_vertihub_popularity, reverse=True)), file=_file)\n",
    "    \n",
    "    def load_vertiport_data(display_stats=True, _file=sys.stdout):\n",
    "        \n",
    "        vertiports_data = pd.read_csv(VERTIPORTS_PATH)\n",
    "\n",
    "        if display_stats:\n",
    "            vertiport_statistics(vertiports_data, _file)\n",
    "\n",
    "        return vertiports_data\n",
    "    def load_trip_data(vertiports_data, display_stats=True, _file=sys.stdout):\n",
    "\n",
    "        trip_data = pd.read_csv(TRIPS_PATH)\n",
    "        trip_data.drop(trip_data[trip_data.OriginVertihub == trip_data.DestinationVertihub].index, inplace=True) #drop trips where the origin and destination are the same vertiport, reduced from 4801 trips to 1692 with 10 vertiports\n",
    "        if display_stats:\n",
    "            trip_statistics(trip_data, vertiports_data, _file)\n",
    "        return trip_data\n",
    "    \n",
    "    def format_purdue_dataset():\n",
    "        vertiports_data = load_vertiport_data()\n",
    "        trip_data = load_trip_data(vertiports_data)\n",
    "        arrivals_per_vertihub = [[] for i in range(NUM_VERTIHUBS)]\n",
    "        # get the trip arrival time per vertihub\n",
    "        latest_arrival_time = -1\n",
    "        for index, trip in trip_data.iterrows():\n",
    "            #calculate time of arrival (begin at trip.time), calculated between vertiports\n",
    "            origin_port = vertiports_data.iloc[trip['OriginVertiport']]\n",
    "            destination_port = vertiports_data.iloc[trip['DestinationVertiport']]\n",
    "            #trip distance in kilometers\n",
    "            dist = ll_distance(origin_port['lat'], origin_port['long'], destination_port['lat'], destination_port['long'])\n",
    "            travel_time = (dist*1000)/FLIGHT_SPEED # (km * 1000)/(m/s) -> seconds\n",
    "            arrival_time = trip['Time'] + travel_time #takeoff time + travel_time = arrival time\n",
    "            if(arrival_time > latest_arrival_time):\n",
    "                latest_arrival_time = arrival_time\n",
    "            #add arrival time to vertihub\n",
    "            # arrivals_per_vertihub[trip['DestinationVertihub']].append((\n",
    "            #     int(arrival_time), \n",
    "            #     int(destination_port['vertihub']), \n",
    "            #     int(destination_port['vertiport']\n",
    "            # )))\n",
    "            arrivals_per_vertihub[trip['DestinationVertihub']].append((\n",
    "                int(arrival_time), \n",
    "                int(trip['DestinationVertihub']), \n",
    "                int(destination_port['vertiport']\n",
    "            )))\n",
    "        #build input list\n",
    "        input = [[dict()] for i in range(int(latest_arrival_time)+1)]\n",
    "        for arrivals in arrivals_per_vertihub:\n",
    "            for arrival in arrivals:\n",
    "                arrival_time, destination_hub, destination_port = arrival\n",
    "                randomized_TTL = random.randint(MIN_TTL, MAX_TTL)\n",
    "                to_add = (('' + str(destination_port)), randomized_TTL) # (destination port, time to land)\n",
    "                if destination_hub not in input[arrival_time][0].keys():\n",
    "                    input[arrival_time][0][destination_hub] = []\n",
    "                #TODO: CHANGE THIS EVENTUALLY!!!!!\n",
    "                if(len(input[arrival_time][0][destination_hub]) < 100): \n",
    "                    input[arrival_time][0][destination_hub].append(to_add)\n",
    "          \n",
    "        #build vertihub list\n",
    "        initial_vertihubs = []\n",
    "        for vertihub_index in range(NUM_VERTIHUBS):\n",
    "            #TODO: fix this... replace 'no_pref' with vertiport_index and figure out the issue with your legacy code! :)\n",
    "            port_dict = {'no_pref' : 3}\n",
    "            # 2/18. you figured out that until you use the actual vertiport index, you can just have one no_pref key (dicts do not allow duplicate keys). before, you were trying to add a ('no_pref' : 3 ) k,v pair for every vertiport. u did not realize that it was just one key value pair that actually went through T_T\n",
    "            # for vertiport_index, vertiport_row in vertiports_data.iterrows():\n",
    "            #   \n",
    "            # port_dict = {'no_pref' : 3 for vertiport_index, vertiport_row in vertiports_data.iterrows() if vertiport_row['vertihub'] == vertihub_index}\n",
    "            accepted_requests_per_time_step = 1\n",
    "            request_vector = []\n",
    "            time_vec = []\n",
    "            vertihub = gm.return_tower_specific(port_dict, accepted_requests_per_time_step, request_vector, time_vec)\n",
    "            initial_vertihubs.append(vertihub)\n",
    "        return initial_vertihubs, input\n",
    "                          \n",
    "                \n",
    "                \n",
    "                                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "data collection functions\n",
    "'''\n",
    "def run_realtime_data_collection(initial_system_copy, input_copy, PDOs=None):\n",
    "    gm.reset_globals()\n",
    "    global MAX_ALLOWED_REQUESTS\n",
    "    \n",
    "    _completed_states_per_tau = [[] for i in range(TAU_MAX)]\n",
    "    _timing_info_per_tau = [[] for i in range(TAU_MAX)]\n",
    "    for _tau in range(TAU_MAX):\n",
    "        rm.configure_realtime(tau=_tau, override_default_empty_state=DEFAULT_EMPTY_STATE)\n",
    "        traces = None\n",
    "        timings = None\n",
    "        if not USE_PURDUE_DATA:\n",
    "            traces, timings = rm.main_loop(initial_system_copy, copy.deepcopy(input_copy))\n",
    "        else:\n",
    "            #NOTE: when using the Purdue data, we will be deleting requests that were denied during runs with higher TAUs.\n",
    "            # that is why we are no longer making a copy of the input_copy, and instead passing the direct reference to be modified\n",
    "            #NOTE: with request queueing, the previous comment is deprecated. all requests will be kept, but a queue will be used in the event of overflow\n",
    "            print(\"INPUT STATISTICS FOR TAU : \" + str(_tau))\n",
    "            generate_input_statistics(input_copy)\n",
    "            curr_PDO = PDOs[_tau]\n",
    "            traces, timings = rm.main_loop(initial_system_copy, copy.deepcopy(input_copy), MAX_ALLOWED_REQUESTS=MAX_ALLOWED_REQUESTS, Purdue_Data_Output=curr_PDO)\n",
    "#             if _tau == 0:\n",
    "            input_copy = curr_PDO.additional_requests_culled\n",
    "#                 MAX_ALLOWED_REQUESTS = 100 #because fuck the next guys\n",
    "        _completed_states_per_tau[_tau] = copy.deepcopy(traces)\n",
    "        _timing_info_per_tau[_tau] = copy.deepcopy(timings)\n",
    "        gm.reset_globals()\n",
    "    return _completed_states_per_tau, _timing_info_per_tau\n",
    "    #     print(traces)\n",
    "        # _completed_states, _timing_info = rm.main_loop(initial_system_copy, input_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/Realtime/OpsLimits/vertiports_16.csv' does not exist: b'data/Realtime/OpsLimits/vertiports_16.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e0426cd8e7ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0m_actual_frequencies_per_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0minitial_system\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_purdue_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Purdue dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;31m# print(_input)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mPDOs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mPurdue_Data_Output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTAU_MAX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-5b82597915f2>\u001b[0m in \u001b[0;36mformat_purdue_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_purdue_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mvertiports_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_vertiport_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0mtrip_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_trip_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvertiports_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0marrivals_per_vertihub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_VERTIHUBS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-5b82597915f2>\u001b[0m in \u001b[0;36mload_vertiport_data\u001b[1;34m(display_stats, _file)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_vertiport_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_stats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mvertiports_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVERTIPORTS_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdisplay_stats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1904\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1906\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1907\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/Realtime/OpsLimits/vertiports_16.csv' does not exist: b'data/Realtime/OpsLimits/vertiports_16.csv'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "data collection pipeline\n",
    "'''\n",
    "import time\n",
    "start_time = time.time()\n",
    "if USE_PURDUE_DATA:\n",
    "    FREQUENCY_MULTIPLIERS = [1]\n",
    "    TAU_MAX = 1 #just run it for the TAU = 0 case.\n",
    "\n",
    "_completed_states_per_tau_per_freq = {freq : [] for freq in FREQUENCY_MULTIPLIERS}\n",
    "_timing_info_per_tau_per_freq = {freq : [] for freq in FREQUENCY_MULTIPLIERS}\n",
    "_actual_frequencies_per_freq = {freq : [] for freq in FREQUENCY_MULTIPLIERS}\n",
    "NUM_TRIALS = 5\n",
    "for freq in FREQUENCY_MULTIPLIERS:\n",
    "    if not USE_PURDUE_DATA:\n",
    "        # setting the frequency multipliers\n",
    "        MAX_LOW_TRAFFIC = LOW_TRAFFIC_MULTIPLIER * int(NUM_VERTIHUBS * freq)\n",
    "    \n",
    "        MIN_HIGH_TRAFFIC = MAX_LOW_TRAFFIC\n",
    "        MAX_HIGH_TRAFFIC = HIGH_TRAFFIC_MULTIPLIER * int(NUM_VERTIHUBS * freq)\n",
    "        for i in range(NUM_TRIALS):\n",
    "            initial_system, input = generate_inputs()\n",
    "            input_frequency = generate_input_statistics(input) \n",
    "            _completed_states_per_tau, _timing_info_per_tau = run_realtime_data_collection(copy.deepcopy(initial_system), copy.deepcopy(input))\n",
    "            _completed_states_per_tau_per_freq[freq].append(_completed_states_per_tau)\n",
    "            _timing_info_per_tau_per_freq[freq].append(_timing_info_per_tau)\n",
    "            _actual_frequencies_per_freq[freq].append(input_frequency)\n",
    "    else:\n",
    "        initial_system, _input = format_purdue_dataset() #Purdue dataset\n",
    "        # print(_input)\n",
    "        PDOs = [Purdue_Data_Output() for i in range(TAU_MAX)]\n",
    "        input_frequency = generate_input_statistics(_input) \n",
    "        _completed_states_per_tau, _timing_info_per_tau = run_realtime_data_collection(copy.deepcopy(initial_system), copy.deepcopy(_input), PDOs=PDOs)\n",
    "        _completed_states_per_tau_per_freq[freq].append(_completed_states_per_tau)\n",
    "        _timing_info_per_tau_per_freq[freq].append(_timing_info_per_tau)\n",
    "        _actual_frequencies_per_freq[freq].append(input_frequency)\n",
    "        \n",
    "        for _tau, pdo in enumerate(PDOs):\n",
    "            print(\"DENIED REQUESTS, MAX_REQUESTS, TAU \" + str(pdo.num_denied_requests) + \", \" + str(pdo.max_requests) + \", \" + str(_tau))\n",
    "end_time = time.time()\n",
    "print('start time = ' + str(start_time) + ' end time = ' + str(end_time) + ' diff = ' + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_TRIALS = 5\n",
    "if USE_PURDUE_DATA:\n",
    "    NUM_TRIALS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data buffer\n",
    "'''\n",
    "completed_states_per_tau_per_freq = copy.deepcopy(_completed_states_per_tau_per_freq)\n",
    "timing_info_per_tau_per_freq = copy.deepcopy(_timing_info_per_tau_per_freq)\n",
    "actual_frequencies_per_freq = copy.deepcopy(_actual_frequencies_per_freq)\n",
    "\n",
    "\n",
    "# print (_actual_frequencies_per_freq)\n",
    "# print (_completed_states_per_tau_per_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (sum(timing_info_per_tau_per_freq[1.5][0][0])/len(timing_info_per_tau_per_freq[1.5][0][0]))\n",
    "# print (len(timing_info_per_tau_per_freq[1.5][0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%m_%d_%H%M%S\")\n",
    "folder_path = ('data/Realtime/OpsLimits/' + str(dt_string)) # make folder with the current date time\n",
    "os.mkdir(folder_path)\n",
    "def save_plot(plot, name):\n",
    "    plot.savefig(folder_path + '/' + name + '.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "# if not USE_PURDUE_DATA:\n",
    "#     dill.dump_session(folder_path + '/realtime_notebook_store_highest_freq.db')\n",
    "# else:\n",
    "#     dill.dump_session(folder_path + '/notebook_dump'+str(NUM_VERTIHUBS)+'.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'PDOs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0409d2ab1da6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mPDO_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-0409d2ab1da6>\u001b[0m in \u001b[0;36mPDO_statistics\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mUSE_PURDUE_DATA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mPDO_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mPDO\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPDOs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'requests expired = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPDO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_expired_requests\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'min ttl = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPDO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpired_requests\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PDOs' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PDO analysis\n",
    "'''\n",
    "\n",
    "if USE_PURDUE_DATA:\n",
    "    def PDO_statistics(f=sys.stdout):\n",
    "        for PDO in PDOs:\n",
    "            print('requests expired = ' + str(PDO.num_expired_requests), file=f)\n",
    "            print('min ttl = ' + str(min(PDO.expired_requests)), file=f)\n",
    "            print('max ttl = ' + str(max(PDO.expired_requests)), file=f)    \n",
    "            expiration_times = [req[0] for req in PDO.expired_requests]\n",
    "            print('avg ttl = ' + str(sum(expiration_times)/len(expiration_times)), file=f)\n",
    "            print('avg queue sizes per vertihub = ' + str(PDO.average_queue_size), file=f)\n",
    "            print('avg queue size overall = ' + str(sum(PDO.average_queue_size)/len(PDO.average_queue_size)), file=f)\n",
    "            print('')\n",
    "\n",
    "PDO_statistics()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'end_time' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b47134df6ac4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Run information: \\n\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Date of completion: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%m/%d/%Y %H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mseconds_to_complete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Completed in \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds_to_complete\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" seconds (\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds_to_complete\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60.0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m\" minutes)\\n\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'end_time' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "save metadata for runtime information\n",
    "'''\n",
    "with open(folder_path + \"/run_information.txt\", \"w\") as f:\n",
    "    f.write(\"Run information: \\n\\n\")\n",
    "    f.write(\"Date of completion: \" + now.strftime(\"%m/%d/%Y %H:%M:%S\") + \"\\n\")\n",
    "    seconds_to_complete = end_time - start_time\n",
    "    f.write(\"Completed in \" + str(seconds_to_complete) + \" seconds (\" + str(seconds_to_complete/60.0)  + \" minutes)\\n\\n\") \n",
    "\n",
    "\n",
    "    if USE_PURDUE_DATA:\n",
    "        f.write(\"Purdue Data Information: \\n\"\n",
    "                \"--------------------------\"\n",
    "                \"\\n\"\n",
    "            )\n",
    "\n",
    "\n",
    "        f.write(\"Trips file: \" + str(TRIPS_PATH) + \"\\n\")\n",
    "        f.write(\"Vertiports file: \" + str(VERTIPORTS_PATH) + \"\\n\")\n",
    "        f.write(\"Minutes of data: \" + str(MINUTES_OF_DATA) + \"\\n\")\n",
    "        f.write(\"Additional Vertihubs: \" + str(ADDITIONAL_VERTIHUBS) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Trip data statistics \" )\n",
    "        f.write(\"\\n\")\n",
    "        vertiport_data = load_vertiport_data(True, f)\n",
    "        load_trip_data(vertiport_data, True, f)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Request queue statistics \" )\n",
    "        f.write(\"\\n\")\n",
    "        PDO_statistics(f)\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"End Purdue Data Information: \\n\"\n",
    "                \"--------------------------\"\n",
    "                \"\\n\"\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "percentage of valid states\n",
    "'''\n",
    "def get_percent_valid(completed_states):\n",
    "    percent_valid = [0 for i in range(len(completed_states[0]))]\n",
    "    percent_valid_per_tau = [copy.copy(percent_valid) for i in range(TAU_MAX)]\n",
    "    for tau,completed in enumerate(completed_states):\n",
    "        print(completed)\n",
    "        print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "        for index, com in enumerate(completed):\n",
    "            for state in com:\n",
    "                if(\"VALID\" in state.labels):\n",
    "                    percent_valid_per_tau[tau][index] += 1\n",
    "\n",
    "    for tower_cost in percent_valid_per_tau:\n",
    "        print(tower_cost)\n",
    "    return percent_valid_per_tau\n",
    "    \n",
    "# actual_percentages = [(i/len(completed_states[0])) for i in percent_valid]\n",
    "# print(actual_percentages)\n",
    "'''\n",
    "mvp_output\n",
    "'''\n",
    "def get_mvp_output_per_tower_per_tau(completed_states):\n",
    "    mvp_output_per_tower_per_tau = [rm.get_mvp_output(completed) for completed in completed_states]\n",
    "    for tau in mvp_output_per_tower_per_tau:\n",
    "        for output in tau:\n",
    "            gm.print_formatted_cost(output[0],format_override=True)\n",
    "    #     output[3].plot()\n",
    "    #     gm.print_formatted_trace_path(output[1])\n",
    "    return mvp_output_per_tower_per_tau\n",
    "\n",
    "'''\n",
    "calculating heuristic cost\n",
    "'''\n",
    "def get_heuristic_cost_per_tau(completed_states):\n",
    "    heurstic_cost_per_tau = [copy.copy(percent_valid) for i in range(TAU_MAX)]\n",
    "    for tau,completed in enumerate(completed_states):\n",
    "        for index, com in enumerate(completed):\n",
    "            for state in com:\n",
    "                for req in state.request_vector:\n",
    "                    if(req == \"wrong_tower\"):\n",
    "                        heurstic_cost_per_tau[tau][index] += 1\n",
    "    for heuristic_cost in heurstic_cost_per_tau:\n",
    "        print(heuristic_cost)\n",
    "    return heuristic_cost_per_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "table for runtime data\n",
    "\n",
    "most important data is synthesis per time per timestep. \n",
    "Ill just do runtime vs TAU and frequency, one for each trial\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_runtime_table(timing_info, input_frequencies, trial_num, iteration):\n",
    "    row_headers = [''+str(freq) for freq in input_frequencies]\n",
    "    column_headers = [''+str(tau) for tau in range(TAU_MAX)]\n",
    "    cell_text = []\n",
    "    for freq in FREQUENCY_MULTIPLIERS:\n",
    "        cell_text.append(timing_info[freq])\n",
    "        print(cell_text)\n",
    "#         cell_text.append([0 for i in range(TAU_MAX)])\n",
    "    the_table = plt.table(cellText=cell_text, rowLabels=row_headers, colLabels=column_headers, loc='center')\n",
    "    the_table.scale(1, 1.5)\n",
    "    the_table.set_fontsize(25)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.box(on=None)\n",
    "#     plt.show()\n",
    "#     fig = plt.gcf()\n",
    "    if USE_PURDUE_DATA:\n",
    "        plt.savefig('data/Realtime/OpsLimits/runtime_vs_TAU_and_freq_trial_' + str(trial_num) + '_iteration_' + str(iteration) + '.png',bbox_inches='tight',dpi=216)    \n",
    "    else:\n",
    "        plt.savefig('data/Realtime/runtime_vs_TAU_and_freq_trial_' + str(trial_num) + '_iteration_' + str(iteration) + '.png',bbox_inches='tight',dpi=216)    \n",
    "    \n",
    "    return plt\n",
    "\n",
    "def get_data_per_trial(full_timing_info, full_input_frequencies, trial_index):\n",
    "    stub_timing_info = {freq : [0 for i in range(TAU_MAX)] for freq in FREQUENCY_MULTIPLIERS}\n",
    "    for freq in full_timing_info:        \n",
    "        for TAU in range(len(full_timing_info[freq][trial_index])):\n",
    "            timings_without_zeros = [t for t in full_timing_info[freq][trial_index][TAU] if t != 0]\n",
    "            stub_timing_info[freq][TAU] += sum(timings_without_zeros)/len(timings_without_zeros)\n",
    "    stub_input_frequencies = {freq: 0 for freq in FREQUENCY_MULTIPLIERS}\n",
    "    for freq in full_input_frequencies:\n",
    "        stub_input_frequencies[freq] = full_input_frequencies[freq][trial_index]\n",
    "    return stub_timing_info, stub_input_frequencies\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c64b23ad5bb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0maverage_timing_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mfreq\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTAU_MAX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFREQUENCY_MULTIPLIERS\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_TRIALS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtiming_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data_per_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtiming_info_per_tau_per_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual_frequencies_per_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#     print(timing_info)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtiming_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-4be5990ffba3>\u001b[0m in \u001b[0;36mget_data_per_trial\u001b[1;34m(full_timing_info, full_input_frequencies, trial_index)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mstub_timing_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mfreq\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTAU_MAX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFREQUENCY_MULTIPLIERS\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_timing_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mTAU\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_timing_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mtimings_without_zeros\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_timing_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTAU\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mstub_timing_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTAU\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimings_without_zeros\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimings_without_zeros\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "average_timing_info = {freq : [0 for i in range(TAU_MAX)] for freq in FREQUENCY_MULTIPLIERS}\n",
    "for trial in range(0, NUM_TRIALS):\n",
    "    timing_info, input_freq = get_data_per_trial(timing_info_per_tau_per_freq, actual_frequencies_per_freq, trial)\n",
    "#     print(timing_info)\n",
    "    for key in timing_info:\n",
    "        for index,num in enumerate(timing_info[key]):\n",
    "            timing_info[key][index] = round(num,3)\n",
    "    generate_runtime_table(timing_info, input_freq, trial, 2)\n",
    "#     for key in timing_info:\n",
    "#         for index,num in enumerate(timing_info[key]):\n",
    "#             average_timing_info[key][index] += num\n",
    "            \n",
    "for key in timing_info:\n",
    "    for index,num in enumerate(timing_info[key]):\n",
    "        average_timing_info[key][index] = round(num,3)\n",
    "# fig = generate_runtime_table(average_timing_info, input_freq, trial)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f5edc0f01e80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmvp\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcompleted_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompleted_states_per_tau_per_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpercent_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompleted_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpercent_valid_per_tau\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercent_valid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTAU_MAX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "'''\n",
    "data buffer for mvp output\n",
    "'''\n",
    "completed_states = completed_states_per_tau_per_freq[1][0]\n",
    "percent_valid = [0 for i in range(len(completed_states[0]))]\n",
    "percent_valid_per_tau = [copy.copy(percent_valid) for i in range(TAU_MAX)]\n",
    "# wrong_towers_per_tau = [copy.copy(percent_valid) for i in range(TAU_MAX)]\n",
    "wrong_tower_per_time_step = [0 for i in range(len(completed_states[0][0]))]\n",
    "number_requests_per_TAU = [0 for i in range(len(completed_states[0]))]\n",
    "# print(completed_states[0][0])\n",
    "# print(completed_states_per_tau_per_freq[1][0][0][0][0]) freq, something, tau, tower, state\n",
    "states_per_tau = [0 for i in range(TAU_MAX)]\n",
    "for tau,completed in enumerate(completed_states): # tau\n",
    "    for index, com in enumerate(completed): # vertihub\n",
    "        for time_step, state in enumerate(com):\n",
    "            if(\"VALID\" in state.labels):\n",
    "                percent_valid_per_tau[tau][index] += 1\n",
    "            states_per_tau[tau] += 1\n",
    "            for req in state.request_vector:\n",
    "                if req == 'wrong_tower':\n",
    "                     wrong_tower_per_time_step[time_step] += 1\n",
    "#                      wrong_towers_per_tau[tau][time_step] += 1\n",
    "                    \n",
    "# print(completed_states[0][0])\n",
    "# sums = [sum(i) for i in wrong_towers_per_tau]\n",
    "# print(sums)          \n",
    "other_sums = [sum(i)/states_per_tau[tau] for tau, i in enumerate(percent_valid_per_tau)]\n",
    "print(other_sums)\n",
    "\n",
    "    \n",
    "\n",
    "# for tower_cost in percent_valid_per_tau:\n",
    "#     print(tower_cost)\n",
    "# mvp_output_per_tower_per_tau_copy = copy.deepcopy(mvp_output_per_tower_per_tau)\n",
    "# for tau in mvp_output_per_tower_per_tau_copy:\n",
    "#     for output in tau:\n",
    "#         gm.print_formatted_cost(output[0],format_override=True)\n",
    "# for timings in timing_info:\n",
    "#     print(sum(timings))\n",
    "#     print (timings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'completed_states' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-63040e497d9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# sum up the negative costs for the tower (get total expiration value)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcost_per_tau_over_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTAU_MAX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtau\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompleted_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msum_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompleted_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'completed_states' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "More data processing\n",
    "plot cost per tau over time\n",
    "'''\n",
    "# sum up the negative costs for the tower (get total expiration value)\n",
    "cost_per_tau_over_time = [[] for i in range(TAU_MAX)]\n",
    "for index,tau in enumerate(completed_states):\n",
    "    sum_cost = 0\n",
    "    for time_step in range(len(completed_states[0][0])):\n",
    "        for tower in tau:\n",
    "            for expiration in tower[time_step].time_vector:\n",
    "                if expiration < 0:\n",
    "                    sum_cost -= expiration \n",
    "#                     sum_cost +=1\n",
    "        cost_per_tau_over_time[index].append(sum_cost)\n",
    "cost_per_tau_over_time = cost_per_tau_over_time\n",
    "# print(cost_per_tau_over_time)\n",
    "x = [i for i in range(len(completed_states[0][0]))]\n",
    "plt.plot(x, cost_per_tau_over_time[0])\n",
    "# for tau_num, tau in enumerate(cost_per_tau_over_time):\n",
    "#     plt.plot(x, tau, label = \"TAU = \" + str(tau_num))\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Cumulative Expiration Error')\n",
    "save_plot(plt, 'cummulative_expiration_error_over_time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'completed_states' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a53e7ff45f74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdo\u001b[0m \u001b[0ma\u001b[0m \u001b[0mline\u001b[0m \u001b[0mper\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m '''\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompleted_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mcummulative_wrong_tower_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrong_tower_per_time_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrong_tower_per_time_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'completed_states' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "first plot:\n",
    "cummulative expiration cost vs. time step\n",
    "label with periods of high traffic and low traffic\n",
    "do a line per tau\n",
    "'''\n",
    "x = [i+1 for i in range(len(completed_states[0][0]))]\n",
    "cummulative_wrong_tower_error = [0 for i in range(len(wrong_tower_per_time_step))]\n",
    "for idx, i in enumerate(wrong_tower_per_time_step):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    cummulative_wrong_tower_error[idx] = i + cummulative_wrong_tower_error[idx-1]\n",
    "print(wrong_tower_per_time_step)\n",
    "plt.plot(x, cummulative_wrong_tower_error)\n",
    "# for tau_num, tau in enumerate(cost_per_tau_over_time):\n",
    "#     plt.plot(x, tau, label = \"TAU = \" + str(tau_num))\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Cumulative Swap Error')\n",
    "save_plot(plt, 'cummulative_swap_error_over_time')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d32f97d8a6ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0malso\u001b[0m \u001b[0minclude\u001b[0m \u001b[0ma\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraffic\u001b[0m \u001b[0mover\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m '''\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcum_input_len_over_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "second plot:\n",
    "cummulative expiration cost vs. time step\n",
    "label with periods of high traffic and low traffic\n",
    "do a line per tau\n",
    "also include a line for the traffic over time\n",
    "'''\n",
    "cum_input_len_over_time = [0 for i in x]\n",
    "for time_step in range(len(_input)):\n",
    "    if time_step > 0:\n",
    "        cum_input_len_over_time[time_step] = cum_input_len_over_time[time_step-1]  \n",
    "    for key in _input[time_step][0].keys():\n",
    "        cum_input_len_over_time[time_step] += len(_input[time_step][0][key])\n",
    "print (cum_input_len_over_time)\n",
    "for i in range(len(_input), len(x)):\n",
    "    cum_input_len_over_time[i] = cum_input_len_over_time[i-1]\n",
    "\n",
    "fig,ax1 = plt.subplots()\n",
    "# print (cost_per_tau_over_time[4])\n",
    "for tau_num, tau in enumerate(cost_per_tau_over_time):\n",
    "    ax1.plot(x, tau, label = \"TAU = \" + str(tau_num))\n",
    "plt.legend()\n",
    "ax1.set_xlabel('Time Step')\n",
    "ax1.set_ylabel('Cumulative Error')\n",
    "input_len_over_time = [0 for i in x]\n",
    "for time_step in range(len(_input)):\n",
    "    for key in _input[time_step][0].keys():\n",
    "        input_len_over_time[time_step] += len(_input[time_step][0][key])\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x, input_len_over_time, color='r', alpha=0.1)\n",
    "# ax2.plot(x, cum_input_len_over_time, 'r.')\n",
    "ax2.set_ylabel('Additional Requests')\n",
    "ax2.tick_params('y',colors='r')\n",
    "    # plt.savefig('data/Realtime/OpsLimits/cumulative_error_over_time_with_requests' + str(random.randrange(0,100000)) + '.png',dpi=216)    \n",
    "save_plot(plt, 'cumulative_error_over_time_with_requests')\n",
    "\n",
    "    # plt.savefig('data/Realtime/cumulative_error_over_time_with_requests' + str(random.randrange(0,100000)) + '.png',dpi=216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-c054876ce982>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtraffic\u001b[0m \u001b[0mover\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m '''\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0minput_len_over_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "second plot:\n",
    "traffic over time\n",
    "'''\n",
    "input_len_over_time = [0 for i in x]\n",
    "for time_step in range(len(input)):\n",
    "    for key in input[time_step][0].keys():\n",
    "        input_len_over_time[time_step] += len(input[time_step][0][key])\n",
    "plt.bar(x, input_len_over_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'timing_info' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-fb7eea000afa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtimings\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtimings\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtiming_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#     print(timings)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timing_info' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "timings data\n",
    "'''\n",
    "for timings in timing_info:\n",
    "#     print(timings)\n",
    "    print(sum(timings))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "062ca1eb9d1cfd384bf82597815a492c2555446b36a8d1e8dd41731a23537749"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}